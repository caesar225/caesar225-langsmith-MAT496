{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith Key Set: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "import os\n",
    "print(\"LangSmith Key Set:\", os.getenv(\"LANGCHAIN_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\DELL\\Desktop\\Jupyter\\mat\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching pages: 100%|#############################################################################| 197/197 [00:32<00:00,  6.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To set up tracing to LangSmith with @traceable, you need to set the LANGSMITH_TRACING environment variable to 'true' and the LANGSMITH_API_KEY environment variable to your API key. You can then use the @traceable decorator in Python to log traces to LangSmith. Simply decorate any function with @traceable to log traces, and use the await keyword when calling wrapped sync functions to ensure correct logging.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset to evaluate this particular step of our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset 'LangSmith Q&A Updated' now has 5 examples.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_dataset = [\n",
    "    (\n",
    "        \"How do I enable tracing in a LangChain application?\",\n",
    "        \"LangChain supports native tracing through LangSmith by setting environment variables and wrapping chains or tools with decorators.\",\n",
    "        \"To enable tracing in a LangChain app, set the environment variable `LANGSMITH_TRACING=true`, and ensure your `LANGSMITH_API_KEY` is set. Optionally, wrap custom components with the `@traceable` decorator.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is the difference between LangChain and LangSmith?\",\n",
    "        \"LangChain is a framework for building LLM applications. LangSmith is a developer platform for debugging, testing, and monitoring those applications.\",\n",
    "        \"LangChain provides abstractions like chains, agents, and tools for LLM apps. LangSmith offers observability, evaluation, and dataset management to monitor and improve them.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How do I log custom LLM calls in LangSmith?\",\n",
    "        \"LangSmith provides the `RunTree` class, which allows you to manually construct and submit traces for custom logic or non-standard LLM calls.\",\n",
    "        \"You can use `RunTree` to manually create and log a trace:\\n\\n```python\\nfrom langsmith.run_trees import RunTree\\n\\nrun = RunTree(name='Custom LLM Call', run_type='llm', inputs={'prompt': '...your prompt...'})\\n# Run your LLM call here\\nrun.end(outputs={'response': '...your output...'})\\nrun.postRun()\\n```\"\n",
    "    ),\n",
    "    (\n",
    "        \"Can I evaluate prompt templates in LangSmith?\",\n",
    "        \"LangSmith lets you evaluate prompts by linking them to datasets and running chains or LLMs against example inputs.\",\n",
    "        \"Yes. You can upload a prompt template to Prompt Hub, hydrate it with inputs from a dataset, and evaluate its performance using built-in or custom evaluators.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How do I create and share prompt templates in LangSmith?\",\n",
    "        \"The Prompt Hub lets you create, version, and share prompts with others. Templates can be used in LangChain directly via `load_prompt()`.\",\n",
    "        \"To share a prompt, create it in the LangSmith UI under Prompt Hub, then toggle the visibility to public or share the unique prompt URL with your team.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"LangSmith Q&A Updated\"\n",
    "\n",
    "\n",
    "try:\n",
    "    dataset = client.read_dataset(dataset_name)\n",
    "    print(f\"Dataset '{dataset_name}' already exists. Using the existing one.\")\n",
    "except:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Updated technical questions about LangSmith and LangChain\"\n",
    "    )\n",
    "\n",
    "# Prepare inputs and outputs\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "# Create examples\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n",
    "\n",
    "print(f\" Dataset '{dataset_name}' now has {len(inputs)} examples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our Application to use Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
    "\n",
    "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
    "\n",
    "Let's add the code snippet that will pull down our prompt that we just iterated on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "dataset = client.read_dataset(dataset_name=\"LangSmith Q&A Updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langsmith import traceable\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"  # Groq-supported model\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "nest_asyncio.apply()\n",
    "groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    embd = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    loader = SitemapLoader(\n",
    "        web_path=\"https://docs.smith.langchain.com/sitemap.xml\",\n",
    "        continue_on_failure=True\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"llm\", metadata={\"ls_provider\": MODEL_PROVIDER, \"ls_model_name\": MODEL_NAME})\n",
    "def call_groq(messages: List[dict]) -> str:\n",
    "    return groq_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    formatted_prompt = prompt.invoke({\n",
    "        \"language\": \"English\",\n",
    "        \"question\": question,\n",
    "    })\n",
    "\n",
    "    messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    return call_groq(messages)\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"life\")  # replace with your prompt name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = prompt.invoke({\n",
    "    \"language\": \"Greek\",  # or any language you want here\n",
    "    \"question\": question,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = langsmith_rag(question)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
