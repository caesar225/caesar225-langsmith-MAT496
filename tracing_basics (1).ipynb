{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you set your environment variables, including your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith Key Set: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "import os\n",
    "print(\"LangSmith Key Set:\", os.getenv(\"LANGCHAIN_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing with @traceable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\n",
    "\n",
    "The decorator works by creating a run tree for you each time the function is called and inserting it within the current trace. The function inputs, name, and other information is then streamed to LangSmith. If the function raises an error or if it returns a response, that information is also added to the tree, and updates are patched to LangSmith so you can detect and diagnose sources of errors. This is all done on a background thread to avoid blocking your app's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Import necessary modules\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "# ✅ Configuration\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatGroq(model=MODEL_NAME)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        SystemMessage(content=RAG_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=f\"Context: {formatted_docs} \\n\\n Question: {question}\")\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "def call_groq(messages: List, temperature: float = 0.0):\n",
    "    return llm.invoke(messages)\n",
    "\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@traceable handles the RunTree lifecycle for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the process of designing and optimizing prompts to guide the behavior of language models, allowing users to change the way the model responds without modifying its underlying capabilities. It is useful because it provides a simple and high-ROI way to influence the model's behavior, and is often more accessible than other methods like fine-tuning. By crafting effective prompts, users can shape the model's output and achieve their desired results.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is prompt engineering and how is it useful?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith supports sending arbitrary metadata along with traces.\n",
    "\n",
    "Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from typing import List\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Groq client\n",
    "llm = ChatGroq(model=MODEL_NAME)\n",
    "retriever = get_vector_db_retriever()  # assumes you've already defined this\n",
    "\n",
    "@traceable(\n",
    "    metadata={\"vectordb\": \"sklearn\"}\n",
    ")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        SystemMessage(content=RAG_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=f\"Context: {formatted_docs} \\n\\n Question: {question}\")\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "@traceable(\n",
    "    metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER}\n",
    ")\n",
    "def call_groq(\n",
    "    messages: List, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return llm.invoke(messages)\n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace, such as the environment in which it was executed or the user who initiated it. It is useful for associating extra information with a trace, and can be added via the UI or SDK.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is metadata?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add metadata at runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know how electrical engineering works based on the provided context. The context appears to be related to LangSmith, prompt engineering, and large language models, but does not mention electrical engineering.\n"
     ]
    }
   ],
   "source": [
    "question = \"How does electrical engineering work?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
